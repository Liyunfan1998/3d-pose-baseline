{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T03:23:58.676668Z",
     "start_time": "2020-11-20T03:23:54.066547Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liyunfan/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/liyunfan/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/liyunfan/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/liyunfan/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/liyunfan/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/liyunfan/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Predicting 3d poses from 2d joints\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import h5py\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "# import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "import procrustes\n",
    "\n",
    "import viz\n",
    "import cameras\n",
    "import data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T03:12:02.098500Z",
     "start_time": "2020-11-20T03:12:02.035138Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "FLAGS = tf.app.flags.FLAGS\n",
    "lst = list(FLAGS._flags().keys())\n",
    "for key in lst:\n",
    "    FLAGS.__delattr__(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T03:23:59.023300Z",
     "start_time": "2020-11-20T03:23:58.943604Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "tf.app.flags.DEFINE_string('f','','kernel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T03:23:59.328011Z",
     "start_time": "2020-11-20T03:23:59.176176Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiments/All/dropout_1.0/epochs_1/lr_0.001/not_residual/depth_2/linear_size1024/batch_size_64/no_procrustes/no_maxnorm/no_batch_normalization/not_stacked_hourglass/predict_17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.app.flags.DEFINE_float(\"learning_rate\", 1e-3, \"Learning rate\")\n",
    "tf.app.flags.DEFINE_float(\"dropout\", 1, \"Dropout keep probability. 1 means no dropout\")\n",
    "tf.app.flags.DEFINE_integer(\"batch_size\", 64, \"Batch size to use during training\")\n",
    "tf.app.flags.DEFINE_integer(\"epochs\", 1, \"How many epochs we should train for\")\n",
    "tf.app.flags.DEFINE_boolean(\"camera_frame\", False, \"Convert 3d poses to camera coordinates\")\n",
    "tf.app.flags.DEFINE_boolean(\"max_norm\", False, \"Apply maxnorm constraint to the weights\")\n",
    "tf.app.flags.DEFINE_boolean(\"batch_norm\", False, \"Use batch_normalization\")\n",
    "\n",
    "# Data loading\n",
    "tf.app.flags.DEFINE_boolean(\"predict_14\", False, \"predict 14 joints\")\n",
    "tf.app.flags.DEFINE_boolean(\"use_sh\", False, \"Use 2d pose predictions from StackedHourglass\")\n",
    "tf.app.flags.DEFINE_string(\"action\", \"All\", \"The action to train on. 'All' means all the actions\")\n",
    "\n",
    "# Architecture\n",
    "tf.app.flags.DEFINE_integer(\"linear_size\", 1024, \"Size of each model layer.\")\n",
    "tf.app.flags.DEFINE_integer(\"num_layers\", 2, \"Number of layers in the model.\")\n",
    "tf.app.flags.DEFINE_boolean(\"residual\", False, \"Whether to add a residual connection every 2 layers\")\n",
    "\n",
    "# Evaluation\n",
    "tf.app.flags.DEFINE_boolean(\"procrustes\", False, \"Apply procrustes analysis at test time\")\n",
    "tf.app.flags.DEFINE_boolean(\"evaluateActionWise\", False, \"The dataset to use either h36m or heva\")\n",
    "\n",
    "# Directories\n",
    "# tf.app.flags.DEFINE_string(\"cameras_path\",\"data/h36m/cameras.h5\",\"Directory to load camera parameters\")\n",
    "tf.app.flags.DEFINE_string(\"cameras_path\", \"data/Release-v1.2/metadata.xml\", \"Directory to load camera parameters\")\n",
    "tf.app.flags.DEFINE_string(\"csv_dir\", \"../oph36m/\", \"Openpose data directory\")\n",
    "tf.app.flags.DEFINE_string(\"data_dir\", \"data/h36m/\", \"Data directory\")\n",
    "\n",
    "tf.app.flags.DEFINE_string(\"train_dir\", \"experiments\", \"Training directory.\")\n",
    "\n",
    "# openpose\n",
    "tf.app.flags.DEFINE_string(\"pose_estimation_json\", \"/tmp/\",\n",
    "                           \"pose estimation json output directory, openpose or tf-pose-estimation\")\n",
    "# openpose\n",
    "tf.app.flags.DEFINE_string(\"json\", \"/tmp/1.json\",\n",
    "                           \"pose estimation json from openpose\")\n",
    "tf.app.flags.DEFINE_boolean(\"interpolation\", False, \"interpolate openpose json\")\n",
    "tf.app.flags.DEFINE_float(\"multiplier\", 0.1, \"interpolation frame range\")\n",
    "tf.app.flags.DEFINE_boolean(\"write_gif\", False, \"write final anim gif\")\n",
    "tf.app.flags.DEFINE_integer(\"gif_fps\", 30, \"output gif framerate\")\n",
    "tf.app.flags.DEFINE_integer(\"verbose\", 2, \"0:Error, 1:Warning, 2:INFO*(default), 3:debug\")\n",
    "tf.app.flags.DEFINE_boolean(\"cache_on_fail\", True, \"caching last valid frame on invalid frame\")\n",
    "\n",
    "# Train or load\n",
    "tf.app.flags.DEFINE_boolean(\"sample\", False, \"Set to True for sampling.\")\n",
    "tf.app.flags.DEFINE_boolean(\"use_cpu\", False, \"Whether to use the CPU\")\n",
    "tf.app.flags.DEFINE_integer(\"load\", 0, \"Try to load a previous checkpoint.\")\n",
    "\n",
    "# Misc\n",
    "tf.app.flags.DEFINE_boolean(\"use_fp16\", False, \"Train using fp16 instead of fp32.\")\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "train_dir = os.path.join(FLAGS.train_dir,\n",
    "                         FLAGS.action,\n",
    "                         'dropout_{0}'.format(FLAGS.dropout),\n",
    "                         'epochs_{0}'.format(FLAGS.epochs) if FLAGS.epochs > 0 else '',\n",
    "                         'lr_{0}'.format(FLAGS.learning_rate),\n",
    "                         'residual' if FLAGS.residual else 'not_residual',\n",
    "                         'depth_{0}'.format(FLAGS.num_layers),\n",
    "                         'linear_size{0}'.format(FLAGS.linear_size),\n",
    "                         'batch_size_{0}'.format(FLAGS.batch_size),\n",
    "                         'procrustes' if FLAGS.procrustes else 'no_procrustes',\n",
    "                         'maxnorm' if FLAGS.max_norm else 'no_maxnorm',\n",
    "                         'batch_normalization' if FLAGS.batch_norm else 'no_batch_normalization',\n",
    "                         'use_stacked_hourglass' if FLAGS.use_sh else 'not_stacked_hourglass',\n",
    "                         'predict_14' if FLAGS.predict_14 else 'predict_17')\n",
    "\n",
    "print(train_dir)\n",
    "summaries_dir = os.path.join(train_dir, \"log\")  # Directory for TB summaries\n",
    "\n",
    "# To avoid race conditions: https://github.com/tensorflow/tensorflow/issues/7448\n",
    "os.system('mkdir -p {}'.format(summaries_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T03:12:14.045928Z",
     "start_time": "2020-11-20T03:12:13.988682Z"
    },
    "code_folding": [],
    "pycharm": {
     "name": "#%%\n"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def create_model(session, actions, batch_size):\n",
    "    \"\"\"\n",
    "    Create model and initialize it or load its parameters in a session\n",
    "\n",
    "    Args\n",
    "      session: tensorflow session\n",
    "      actions: list of string. Actions to train/test on\n",
    "      batch_size: integer. Number of examples in each batch\n",
    "    Returns\n",
    "      model: The created (or loaded) model\n",
    "    Raises\n",
    "      ValueError if asked to load a model, but the checkpoint specified by\n",
    "      FLAGS.load cannot be found.\n",
    "    \"\"\"\n",
    "\n",
    "    model = linear_model.LinearModel(\n",
    "        FLAGS.linear_size,\n",
    "        FLAGS.num_layers,\n",
    "        FLAGS.residual,\n",
    "        FLAGS.batch_norm,\n",
    "        FLAGS.max_norm,\n",
    "        batch_size,\n",
    "        FLAGS.learning_rate,\n",
    "        summaries_dir,\n",
    "        FLAGS.predict_14,\n",
    "        dtype=tf.float16 if FLAGS.use_fp16 else tf.float32)\n",
    "\n",
    "    if FLAGS.load <= 0:\n",
    "        # Create a new model from scratch\n",
    "        print(\"Creating model with fresh parameters.\")\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        return model\n",
    "\n",
    "    # Load a previously saved model\n",
    "    ckpt = tf.train.get_checkpoint_state(train_dir, latest_filename=\"checkpoint\")\n",
    "    print(\"train_dir\", train_dir)\n",
    "\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        # Check if the specific checkpoint exists\n",
    "        if FLAGS.load > 0:\n",
    "            if os.path.isfile(os.path.join(train_dir, \"checkpoint-{0}.index\".format(FLAGS.load))):\n",
    "                ckpt_name = os.path.join(os.path.join(train_dir, \"checkpoint-{0}\".format(FLAGS.load)))\n",
    "            else:\n",
    "                raise ValueError(\"Asked to load checkpoint {0}, but it does not seem to exist\".format(FLAGS.load))\n",
    "        else:\n",
    "            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "\n",
    "        print(\"Loading model {0}\".format(ckpt_name))\n",
    "        model.saver.restore(session, ckpt.model_checkpoint_path)\n",
    "        return model\n",
    "    else:\n",
    "        print(\"Could not find checkpoint. Aborting.\")\n",
    "        raise (ValueError, \"Checkpoint {0} does not seem to exist\".format(ckpt.model_checkpoint_path))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sample():\n",
    "    \"\"\"Get samples from a model and visualize them\"\"\"\n",
    "\n",
    "    actions = data_utils.define_actions(FLAGS.action)\n",
    "    '''data_dir = '/home/lyunfan/NYU_summer_intern/3d_pose_baseline_pytorch/data/'\n",
    "    stat_3d = torch.load(os.path.join(data_dir, 'stat_3d.pth.tar'))\n",
    "    test_set_3d = torch.load(os.path.join(data_dir, 'test_3d.pth.tar'))\n",
    "    test_set_2d = torch.load(os.path.join(data_dir, 'test_2d.pth.tar'))\n",
    "    test_root_positions = stat_3d['test_root']\n",
    "\n",
    "    complete_train = np.vstack(test_set_2d.values())\n",
    "    data_mean_2d, data_std_2d, dim_to_ignore_2d, dim_to_use_2d = data_utils.normalization_stats(complete_train, dim=2)\n",
    "    data_mean_3d, data_std_3d, dim_to_ignore_3d, dim_to_use_3d = stat_3d['mean'], stat_3d['std'], stat_3d['dim_ignore'], \\\n",
    "                                                                 stat_3d['dim_use'],'''\n",
    "    # Load camera parameters\n",
    "    SUBJECT_IDS = [1, 5, 6, 7, 8, 9, 11]\n",
    "    rcams = cameras.load_cameras(FLAGS.cameras_path, SUBJECT_IDS)\n",
    "\n",
    "    # Load 3d data and load (or create) 2d projections\n",
    "    train_set_3d, test_set_3d, data_mean_3d, data_std_3d, dim_to_ignore_3d, dim_to_use_3d, train_root_positions, test_root_positions = data_utils.read_3d_data(\n",
    "        actions, FLAGS.data_dir, FLAGS.camera_frame, rcams, FLAGS.predict_14)\n",
    "\n",
    "    train_set_2d, test_set_2d, data_mean_2d, data_std_2d, dim_to_ignore_2d, dim_to_use_2d = data_utils.create_2d_data(\n",
    "        actions, FLAGS.data_dir, rcams)\n",
    "    print(\"done reading and normalizing data.\")\n",
    "\n",
    "    device_count = {\"GPU\": 0} if FLAGS.use_cpu else {\"GPU\": 1}\n",
    "    with tf.Session(config=tf.ConfigProto(device_count=device_count)) as sess:\n",
    "        # === Create the model ===\n",
    "        print(\"Creating %d layers of %d units.\" % (FLAGS.num_layers, FLAGS.linear_size))\n",
    "        batch_size = 128\n",
    "        model = create_model(sess, actions, batch_size)\n",
    "        print(\"Model loaded\")\n",
    "\n",
    "        for key2d in test_set_2d.keys():\n",
    "\n",
    "            (subj, b, fname) = key2d\n",
    "            print(\"Subject: {}, action: {}, fname: {}\".format(subj, b, fname))\n",
    "\n",
    "            # keys should be the same if 3d is in camera coordinates\n",
    "            key3d = key2d if FLAGS.camera_frame else (subj, b, '{0}.h5'.format(fname.split('.')[0]))\n",
    "            key3d = (subj, b, fname[:-3]) if (fname.endswith('-sh')) and FLAGS.camera_frame else key3d\n",
    "\n",
    "            enc_in = test_set_2d[key2d]\n",
    "            n2d, _ = enc_in.shape\n",
    "            dec_out = test_set_3d[key3d]\n",
    "            n3d, _ = dec_out.shape\n",
    "            assert n2d == n3d\n",
    "\n",
    "            # Split into about-same-size batches\n",
    "            enc_in = np.array_split(enc_in, n2d // batch_size)\n",
    "            dec_out = np.array_split(dec_out, n3d // batch_size)\n",
    "            all_poses_3d = []\n",
    "\n",
    "            for bidx in range(len(enc_in)):\n",
    "                # Dropout probability 0 (keep probability 1) for sampling\n",
    "                dp = 1.0\n",
    "                _, _, poses3d = model.step(sess, enc_in[bidx], dec_out[bidx], dp, isTraining=False)\n",
    "\n",
    "                # denormalize\n",
    "                enc_in[bidx] = data_utils.unNormalizeData(enc_in[bidx], data_mean_2d, data_std_2d, dim_to_ignore_2d)\n",
    "                dec_out[bidx] = data_utils.unNormalizeData(dec_out[bidx], data_mean_3d, data_std_3d, dim_to_ignore_3d)\n",
    "                poses3d = data_utils.unNormalizeData(poses3d, data_mean_3d, data_std_3d, dim_to_ignore_3d)\n",
    "                all_poses_3d.append(poses3d)\n",
    "\n",
    "            # Put all the poses together\n",
    "            enc_in, dec_out, poses3d = map(np.vstack, [enc_in, dec_out, all_poses_3d])\n",
    "\n",
    "            # Convert back to world coordinates\n",
    "            if FLAGS.camera_frame:\n",
    "                N_CAMERAS = 4\n",
    "                # N_JOINTS_H36M = 32\n",
    "                N_JOINTS_H36M = 24\n",
    "\n",
    "                # Add global position back\n",
    "                dec_out = dec_out + np.tile(test_root_positions[key3d], [1, N_JOINTS_H36M])\n",
    "\n",
    "                # Load the appropriate camera\n",
    "                subj, _, sname = key3d\n",
    "\n",
    "                cname = sname.split('.')[1]  # <-- camera name\n",
    "                scams = {(subj, c + 1): rcams[(subj, c + 1)] for c in range(N_CAMERAS)}  # cams of this subject\n",
    "                scam_idx = [scams[(subj, c + 1)][-1] for c in range(N_CAMERAS)].index(cname)  # index of camera used\n",
    "                the_cam = scams[(subj, scam_idx + 1)]  # <-- the camera used\n",
    "                R, T, f, c, k, p, name = the_cam\n",
    "                assert name == cname\n",
    "\n",
    "                def cam2world_centered(data_3d_camframe):\n",
    "                    data_3d_worldframe = cameras.camera_to_world_frame(data_3d_camframe.reshape((-1, 3)), R, T)\n",
    "                    data_3d_worldframe = data_3d_worldframe.reshape((-1, N_JOINTS_H36M * 3))\n",
    "                    # subtract root translation\n",
    "                    return data_3d_worldframe - np.tile(data_3d_worldframe[:, :3], (1, N_JOINTS_H36M))\n",
    "\n",
    "                # Apply inverse rotation and translation\n",
    "                dec_out = cam2world_centered(dec_out)\n",
    "                poses3d = cam2world_centered(poses3d)\n",
    "\n",
    "    # Grab a random batch to visualize\n",
    "    enc_in, dec_out, poses3d = map(np.vstack, [enc_in, dec_out, poses3d])\n",
    "    idx = np.random.permutation(enc_in.shape[0])\n",
    "    enc_in, dec_out, poses3d = enc_in[idx, :], dec_out[idx, :], poses3d[idx, :]\n",
    "\n",
    "    # Visualize random samples\n",
    "    import matplotlib.gridspec as gridspec\n",
    "\n",
    "    # 1080p\t= 1,920 x 1,080\n",
    "    fig = plt.figure(figsize=(19.2, 10.8))\n",
    "\n",
    "    gs1 = gridspec.GridSpec(5, 9)  # 5 rows, 9 columns\n",
    "    gs1.update(wspace=-0.00, hspace=0.05)  # set the spacing between axes.\n",
    "    plt.axis('off')\n",
    "\n",
    "    subplot_idx, exidx = 1, 1\n",
    "    nsamples = 15\n",
    "    for i in np.arange(nsamples):\n",
    "        # Plot 2d pose\n",
    "        ax1 = plt.subplot(gs1[subplot_idx - 1])\n",
    "        p2d = enc_in[exidx, :]\n",
    "        viz.show2Dpose(p2d, ax1)\n",
    "        ax1.invert_yaxis()\n",
    "\n",
    "        # Plot 3d gt\n",
    "        ax2 = plt.subplot(gs1[subplot_idx], projection='3d')\n",
    "        p3d = dec_out[exidx, :]\n",
    "        viz.show3Dpose(p3d, ax2)\n",
    "\n",
    "        # Plot 3d predictions\n",
    "        ax3 = plt.subplot(gs1[subplot_idx + 1], projection='3d')\n",
    "        p3d = poses3d[exidx, :]\n",
    "        viz.show3Dpose(p3d, ax3, lcolor=\"#9b59b6\", rcolor=\"#2ecc71\")\n",
    "\n",
    "        exidx = exidx + 1\n",
    "        subplot_idx = subplot_idx + 3\n",
    "\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_action_subset(poses_set, action):\n",
    "    \"\"\"\n",
    "    Given a preloaded dictionary of poses, load the subset of a particular action\n",
    "\n",
    "    Args\n",
    "      poses_set: dictionary with keys k=(subject, action, seqname),\n",
    "        values v=(nxd matrix of poses)\n",
    "      action: string. The action that we want to filter out\n",
    "    Returns\n",
    "      poses_subset: dictionary with same structure as poses_set, but only with the\n",
    "        specified action.\n",
    "    \"\"\"\n",
    "    return {k: v for k, v in poses_set.items() if k[1] == action}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_batches(sess, model,\n",
    "                     data_mean_3d, data_std_3d, dim_to_use_3d, dim_to_ignore_3d,\n",
    "                     data_mean_2d, data_std_2d, dim_to_use_2d, dim_to_ignore_2d,\n",
    "                     current_step, encoder_inputs, decoder_outputs, current_epoch=0):\n",
    "    \"\"\"\n",
    "    Generic method that evaluates performance of a list of batches.\n",
    "    May be used to evaluate all actions or a single action.\n",
    "\n",
    "    Args\n",
    "      sess\n",
    "      model\n",
    "      data_mean_3d\n",
    "      data_std_3d\n",
    "      dim_to_use_3d\n",
    "      dim_to_ignore_3d\n",
    "      data_mean_2d\n",
    "      data_std_2d\n",
    "      dim_to_use_2d\n",
    "      dim_to_ignore_2d\n",
    "      current_step\n",
    "      encoder_inputs\n",
    "      decoder_outputs\n",
    "      current_epoch\n",
    "    Returns\n",
    "\n",
    "      total_err\n",
    "      joint_err\n",
    "      step_time\n",
    "      loss\n",
    "    \"\"\"\n",
    "\n",
    "    # n_joints = 17 if not (FLAGS.predict_14) else 14\n",
    "    n_joints = 13\n",
    "    nbatches = len(encoder_inputs)\n",
    "\n",
    "    # Loop through test examples\n",
    "    all_dists, start_time, loss = [], time.time(), 0.\n",
    "    log_every_n_batches = 100\n",
    "    for i in range(nbatches):\n",
    "\n",
    "        if current_epoch > 0 and (i + 1) % log_every_n_batches == 0:\n",
    "            print(\"Working on test epoch {0}, batch {1} / {2}\".format(current_epoch, i + 1, nbatches))\n",
    "\n",
    "        enc_in, dec_out = encoder_inputs[i], decoder_outputs[i]\n",
    "        dp = 1.0  # dropout keep probability is always 1 at test time\n",
    "        step_loss, loss_summary, poses3d = model.step(sess, enc_in, dec_out, dp, isTraining=False)\n",
    "        loss += step_loss\n",
    "\n",
    "        # denormalize\n",
    "        # enc_in = data_utils.unNormalizeData(enc_in, data_mean_2d, data_std_2d, dim_to_ignore_2d)\n",
    "        dec_out = data_utils.unNormalizeData(dec_out, data_mean_3d, data_std_3d, dim_to_ignore_3d)\n",
    "        poses3d = data_utils.unNormalizeData(poses3d, data_mean_3d, data_std_3d, dim_to_ignore_3d)\n",
    "\n",
    "        # Keep only the relevant dimensions\n",
    "        dtu3d = np.hstack((np.arange(3), dim_to_use_3d)) if not (FLAGS.predict_14) else dim_to_use_3d\n",
    "\n",
    "        dec_out = dec_out[:, dtu3d]\n",
    "        poses3d = poses3d[:, dtu3d]\n",
    "\n",
    "        assert dec_out.shape[0] == FLAGS.batch_size\n",
    "        assert poses3d.shape[0] == FLAGS.batch_size\n",
    "\n",
    "        if FLAGS.procrustes:\n",
    "            # Apply per-frame procrustes alignment if asked to do so\n",
    "            for j in range(FLAGS.batch_size):\n",
    "                gt = np.reshape(dec_out[j, :], [-1, 3])\n",
    "                out = np.reshape(poses3d[j, :], [-1, 3])\n",
    "                _, Z, T, b, c = procrustes.compute_similarity_transform(gt, out, compute_optimal_scale=True)\n",
    "                out = (b * out.dot(T)) + c\n",
    "\n",
    "                # poses3d[j, :] = np.reshape(out, [-1, 17 * 3]) if not (FLAGS.predict_14) else np.reshape(out,\n",
    "                #                                                                                         [-1, 14 * 3])\n",
    "                poses3d[j, :] = np.reshape(out, [-1, 13 * 3])\n",
    "\n",
    "        # Compute Euclidean distance error per joint\n",
    "        sqerr = (poses3d - dec_out) ** 2  # Squared error between prediction and expected output\n",
    "        dists = np.zeros((sqerr.shape[0], n_joints))  # Array with L2 error per joint in mm\n",
    "        dist_idx = 0\n",
    "        for k in np.arange(0, n_joints * 3, 3):\n",
    "            # Sum across X,Y, and Z dimenstions to obtain L2 distance\n",
    "            dists[:, dist_idx] = np.sqrt(np.sum(sqerr[:, k:k + 3], axis=1))\n",
    "            dist_idx = dist_idx + 1\n",
    "\n",
    "        all_dists.append(dists)\n",
    "        assert sqerr.shape[0] == FLAGS.batch_size\n",
    "\n",
    "    step_time = (time.time() - start_time) / nbatches\n",
    "    loss = loss / nbatches\n",
    "\n",
    "    all_dists = np.vstack(all_dists)\n",
    "\n",
    "    # Error per joint and total for all passed batches\n",
    "    joint_err = np.mean(all_dists, axis=0)\n",
    "    total_err = np.mean(all_dists)\n",
    "\n",
    "    return total_err, joint_err, step_time, loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "actions = data_utils.define_actions(FLAGS.action)\n",
    "\n",
    "# Load camera parameters\n",
    "SUBJECT_IDS = [1, 5, 6, 7, 8, 9, 11]\n",
    "rcams = cameras.load_cameras(FLAGS.cameras_path, SUBJECT_IDS)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train_set_2d, test_set_2d, data_mean_2d, data_std_2d, dim_to_ignore_2d, dim_to_use_2d = data_utils.create_openpose_2D_data(\n",
    "#     actions, FLAGS.csv_dir, rcams)\n",
    "train_set_2d, test_set_2d, train_frame_seq, test_frame_seq = data_utils.create_openpose_2D_data(actions,\n",
    "                                                                                                FLAGS.csv_dir,\n",
    "                                                                                                rcams)\n",
    "data_mean_2d, data_std_2d, dim_to_use_2d, dim_to_ignore_2d = None, None, None, None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load 3d data and load (or create) 2d projections\n",
    "train_set_3d, test_set_3d, data_mean_3d, data_std_3d, dim_to_ignore_3d, dim_to_use_3d, train_root_positions, test_root_positions = data_utils.read_3d_data(\n",
    "    actions, FLAGS.data_dir, FLAGS.camera_frame, rcams, FLAGS.predict_14)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_set_3d.keys()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def set_3D_used_seq(train_set_3d, test_set_3d, train_frame_seq, test_frame_seq):\n",
    "    train_set_3d_new, test_set_3d_new = {}, {}\n",
    "    for key in set(train_frame_seq.keys()) & set(train_set_3d.keys()):\n",
    "        use = np.array(train_frame_seq[key]) - 1\n",
    "        train_set_3d_new[key] = train_set_3d[key][use]\n",
    "    for key in set(test_frame_seq.keys()) & set(test_set_3d.keys()):\n",
    "        use = np.array(test_frame_seq[key]) - 1\n",
    "        test_set_3d_new[key] = test_set_3d[key][use]\n",
    "    return train_set_3d_new, test_set_3d_new\n",
    "\n",
    "train_set_3d, test_set_3d = set_3D_used_seq(train_set_3d, test_set_3d, train_frame_seq, test_frame_seq)\n",
    "train_set_2d, test_set_2d = train_set_2d[train_set_3d.keys()], test_set_2d[test_set_3d.keys()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import data_utils\n",
    "import linear_model_openpose as linear_model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# Avoid using the GPU if requested\n",
    "device_count = {\"GPU\": 0} if FLAGS.use_cpu else {\"GPU\": 1}\n",
    "with tf.Session(config=tf.ConfigProto(\n",
    "        device_count=device_count,\n",
    "        allow_soft_placement=True)) as sess:\n",
    "\n",
    "    # === Create the model ===\n",
    "    print(\"Creating %d bi-layers of %d units.\" % (FLAGS.num_layers, FLAGS.linear_size))\n",
    "    model = create_model(sess, actions, FLAGS.batch_size)\n",
    "    model.train_writer.add_graph(sess.graph)\n",
    "    print(\"Model created\")\n",
    "\n",
    "    # === This is the training loop ===\n",
    "    step_time, loss, val_loss = 0.0, 0.0, 0.0\n",
    "    current_step = 0 if FLAGS.load <= 0 else FLAGS.load + 1\n",
    "    previous_losses = []\n",
    "\n",
    "    step_time, loss = 0, 0\n",
    "    current_epoch = 0\n",
    "    log_every_n_batches = 100\n",
    "\n",
    "    for _ in xrange(FLAGS.epochs):\n",
    "        current_epoch = current_epoch + 1\n",
    "\n",
    "        # === Load training batches for one epoch ===\n",
    "        encoder_inputs, decoder_outputs = model.get_all_batches(train_set_2d, train_set_3d, FLAGS.camera_frame,\n",
    "                                                                training=True)\n",
    "        nbatches = len(encoder_inputs)\n",
    "        print(\"There are {0} train batches\".format(nbatches))\n",
    "        start_time, loss = time.time(), 0.\n",
    "\n",
    "        # === Loop through all the training batches ===\n",
    "        for i in range(nbatches):\n",
    "\n",
    "            if (i + 1) % log_every_n_batches == 0:\n",
    "                # Print progress every log_every_n_batches batches\n",
    "                print(\"Working on epoch {0}, batch {1} / {2}... \".format(current_epoch, i + 1, nbatches), end=\"\")\n",
    "\n",
    "            enc_in, dec_out = encoder_inputs[i], decoder_outputs[i]\n",
    "            step_loss, loss_summary, lr_summary, _ = model.step(sess, enc_in, dec_out, FLAGS.dropout,\n",
    "                                                                isTraining=True)\n",
    "\n",
    "            if (i + 1) % log_every_n_batches == 0:\n",
    "                # Log and print progress every log_every_n_batches batches\n",
    "                model.train_writer.add_summary(loss_summary, current_step)\n",
    "                model.train_writer.add_summary(lr_summary, current_step)\n",
    "                step_time = (time.time() - start_time)\n",
    "                start_time = time.time()\n",
    "                print(\"done in {0:.2f} ms\".format(1000 * step_time / log_every_n_batches))\n",
    "\n",
    "            loss += step_loss\n",
    "            current_step += 1\n",
    "            # === end looping through training batches ===\n",
    "\n",
    "        loss = loss / nbatches\n",
    "        print(\"=============================\\n\"\n",
    "              \"Global step:         %d\\n\"\n",
    "              \"Learning rate:       %.2e\\n\"\n",
    "              \"Train loss avg:      %.4f\\n\"\n",
    "              \"=============================\" % (model.global_step.eval(),\n",
    "                                                 model.learning_rate.eval(), loss))\n",
    "        # === End training for an epoch ===\n",
    "\n",
    "        # === Testing after this epoch ===\n",
    "        isTraining = False\n",
    "\n",
    "        if FLAGS.evaluateActionWise:\n",
    "\n",
    "            print(\"{0:=^12} {1:=^6}\".format(\"Action\", \"mm\"))  # line of 30 equal signs\n",
    "\n",
    "            cum_err = 0\n",
    "            for action in actions:\n",
    "                print(\"{0:<12} \".format(action), end=\"\")\n",
    "                # Get 2d and 3d testing data for this action\n",
    "                action_test_set_2d = get_action_subset(test_set_2d, action)\n",
    "                action_test_set_3d = get_action_subset(test_set_3d, action)\n",
    "                encoder_inputs, decoder_outputs = model.get_all_batches(action_test_set_2d, action_test_set_3d,\n",
    "                                                                        FLAGS.camera_frame, training=False)\n",
    "\n",
    "                act_err, _, step_time, loss = evaluate_batches(sess, model,\n",
    "                                                               data_mean_3d, data_std_3d, dim_to_use_3d,\n",
    "                                                               dim_to_ignore_3d,\n",
    "                                                               data_mean_2d, data_std_2d, dim_to_use_2d,\n",
    "                                                               dim_to_ignore_2d,\n",
    "                                                               current_step, encoder_inputs, decoder_outputs)\n",
    "                cum_err = cum_err + act_err\n",
    "\n",
    "                print(\"{0:>6.2f}\".format(act_err))\n",
    "\n",
    "            summaries = sess.run(model.err_mm_summary, {model.err_mm: float(cum_err / float(len(actions)))})\n",
    "            model.test_writer.add_summary(summaries, current_step)\n",
    "            print(\"{0:<12} {1:>6.2f}\".format(\"Average\", cum_err / float(len(actions))))\n",
    "            print(\"{0:=^19}\".format(''))\n",
    "\n",
    "        else:\n",
    "            n_joints = 13\n",
    "            encoder_inputs, decoder_outputs = model.get_all_batches(test_set_2d, test_set_3d, FLAGS.camera_frame,\n",
    "                                                                    training=False)\n",
    "\n",
    "            total_err, joint_err, step_time, loss = evaluate_batches(sess, model,\n",
    "                                                                     data_mean_3d, data_std_3d, dim_to_use_3d,\n",
    "                                                                     dim_to_ignore_3d,\n",
    "                                                                     data_mean_2d, data_std_2d, dim_to_use_2d,\n",
    "                                                                     dim_to_ignore_2d,\n",
    "                                                                     current_step, encoder_inputs, decoder_outputs,\n",
    "                                                                     current_epoch)\n",
    "\n",
    "            print(\"=============================\\n\"\n",
    "                  \"Step-time (ms):      %.4f\\n\"\n",
    "                  \"Val loss avg:        %.4f\\n\"\n",
    "                  \"Val error avg (mm):  %.2f\\n\"\n",
    "                  \"=============================\" % (1000 * step_time, loss, total_err))\n",
    "\n",
    "            for i in range(n_joints):\n",
    "                # 6 spaces, right-aligned, 5 decimal places\n",
    "                print(\"Error in joint {0:02d} (mm): {1:>5.2f}\".format(i + 1, joint_err[i]))\n",
    "            print(\"=============================\")\n",
    "\n",
    "            # Log the error to tensorboard\n",
    "            summaries = sess.run(model.err_mm_summary, {model.err_mm: total_err})\n",
    "            model.test_writer.add_summary(summaries, current_step)\n",
    "\n",
    "        # Save the model\n",
    "        print(\"Saving the model... \", end=\"\")\n",
    "        start_time = time.time()\n",
    "        model.saver.save(sess, os.path.join(train_dir, 'checkpoint'), global_step=current_step)\n",
    "        print(\"done in {0:.2f} ms\".format(1000 * (time.time() - start_time)))\n",
    "\n",
    "        # Reset global time and loss\n",
    "        step_time, loss = 0, 0\n",
    "\n",
    "        sys.stdout.flush()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_set_2d.keys()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_action_subset(poses_set, action):\n",
    "    \"\"\"\n",
    "    Given a preloaded dictionary of poses, load the subset of a particular action\n",
    "\n",
    "    Args\n",
    "      poses_set: dictionary with keys k=(subject, action, seqname),\n",
    "        values v=(nxd matrix of poses)\n",
    "      action: string. The action that we want to filter out\n",
    "    Returns\n",
    "      poses_subset: dictionary with same structure as poses_set, but only with the\n",
    "        specified action.\n",
    "    \"\"\"\n",
    "    return {k: v for k, v in poses_set.items() if k[1] == action}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_batches(sess, model,\n",
    "                     data_mean_3d, data_std_3d, dim_to_use_3d, dim_to_ignore_3d,\n",
    "                     data_mean_2d, data_std_2d, dim_to_use_2d, dim_to_ignore_2d,\n",
    "                     current_step, encoder_inputs, decoder_outputs, current_epoch=0):\n",
    "    \"\"\"\n",
    "    Generic method that evaluates performance of a list of batches.\n",
    "    May be used to evaluate all actions or a single action.\n",
    "\n",
    "    Args\n",
    "      sess\n",
    "      model\n",
    "      data_mean_3d\n",
    "      data_std_3d\n",
    "      dim_to_use_3d\n",
    "      dim_to_ignore_3d\n",
    "      data_mean_2d\n",
    "      data_std_2d\n",
    "      dim_to_use_2d\n",
    "      dim_to_ignore_2d\n",
    "      current_step\n",
    "      encoder_inputs\n",
    "      decoder_outputs\n",
    "      current_epoch\n",
    "    Returns\n",
    "\n",
    "      total_err\n",
    "      joint_err\n",
    "      step_time\n",
    "      loss\n",
    "    \"\"\"\n",
    "\n",
    "    # n_joints = 17 if not (FLAGS.predict_14) else 14\n",
    "    n_joints = 13\n",
    "    nbatches = len(encoder_inputs)\n",
    "\n",
    "    # Loop through test examples\n",
    "    all_dists, start_time, loss = [], time.time(), 0.\n",
    "    log_every_n_batches = 100\n",
    "    for i in range(nbatches):\n",
    "\n",
    "        if current_epoch > 0 and (i + 1) % log_every_n_batches == 0:\n",
    "            print(\"Working on test epoch {0}, batch {1} / {2}\".format(current_epoch, i + 1, nbatches))\n",
    "\n",
    "        enc_in, dec_out = encoder_inputs[i], decoder_outputs[i]\n",
    "        dp = 1.0  # dropout keep probability is always 1 at test time\n",
    "        step_loss, loss_summary, poses3d = model.step(sess, enc_in, dec_out, dp, isTraining=False)\n",
    "        loss += step_loss\n",
    "\n",
    "        # denormalize\n",
    "        # enc_in = data_utils.unNormalizeData(enc_in, data_mean_2d, data_std_2d, dim_to_ignore_2d)\n",
    "        dec_out = data_utils.unNormalizeData(dec_out, data_mean_3d, data_std_3d, dim_to_ignore_3d)\n",
    "        poses3d = data_utils.unNormalizeData(poses3d, data_mean_3d, data_std_3d, dim_to_ignore_3d)\n",
    "\n",
    "        # Keep only the relevant dimensions\n",
    "        dtu3d = np.hstack((np.arange(3), dim_to_use_3d)) if not (FLAGS.predict_14) else dim_to_use_3d\n",
    "\n",
    "        dec_out = dec_out[:, dtu3d]\n",
    "        poses3d = poses3d[:, dtu3d]\n",
    "\n",
    "        assert dec_out.shape[0] == FLAGS.batch_size\n",
    "        assert poses3d.shape[0] == FLAGS.batch_size\n",
    "\n",
    "        if FLAGS.procrustes:\n",
    "            # Apply per-frame procrustes alignment if asked to do so\n",
    "            for j in range(FLAGS.batch_size):\n",
    "                gt = np.reshape(dec_out[j, :], [-1, 3])\n",
    "                out = np.reshape(poses3d[j, :], [-1, 3])\n",
    "                _, Z, T, b, c = procrustes.compute_similarity_transform(gt, out, compute_optimal_scale=True)\n",
    "                out = (b * out.dot(T)) + c\n",
    "\n",
    "                # poses3d[j, :] = np.reshape(out, [-1, 17 * 3]) if not (FLAGS.predict_14) else np.reshape(out,\n",
    "                #                                                                                         [-1, 14 * 3])\n",
    "                poses3d[j, :] = np.reshape(out, [-1, 13 * 3])\n",
    "\n",
    "        # Compute Euclidean distance error per joint\n",
    "        sqerr = (poses3d - dec_out) ** 2  # Squared error between prediction and expected output\n",
    "        dists = np.zeros((sqerr.shape[0], n_joints))  # Array with L2 error per joint in mm\n",
    "        dist_idx = 0\n",
    "        for k in np.arange(0, n_joints * 3, 3):\n",
    "            # Sum across X,Y, and Z dimenstions to obtain L2 distance\n",
    "            dists[:, dist_idx] = np.sqrt(np.sum(sqerr[:, k:k + 3], axis=1))\n",
    "            dist_idx = dist_idx + 1\n",
    "\n",
    "        all_dists.append(dists)\n",
    "        assert sqerr.shape[0] == FLAGS.batch_size\n",
    "\n",
    "    step_time = (time.time() - start_time) / nbatches\n",
    "    loss = loss / nbatches\n",
    "\n",
    "    all_dists = np.vstack(all_dists)\n",
    "\n",
    "    # Error per joint and total for all passed batches\n",
    "    joint_err = np.mean(all_dists, axis=0)\n",
    "    total_err = np.mean(all_dists)\n",
    "\n",
    "    return total_err, joint_err, step_time, loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "actions = data_utils.define_actions(FLAGS.action)\n",
    "\n",
    "# Load camera parameters\n",
    "SUBJECT_IDS = [1, 5, 6, 7, 8, 9, 11]\n",
    "rcams = cameras.load_cameras(FLAGS.cameras_path, SUBJECT_IDS)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train_set_2d, test_set_2d, data_mean_2d, data_std_2d, dim_to_ignore_2d, dim_to_use_2d = data_utils.create_openpose_2D_data(\n",
    "#     actions, FLAGS.csv_dir, rcams)\n",
    "train_set_2d, test_set_2d, train_frame_seq, test_frame_seq = data_utils.create_openpose_2D_data(actions,\n",
    "                                                                                                FLAGS.csv_dir,\n",
    "                                                                                                rcams)\n",
    "data_mean_2d, data_std_2d, dim_to_use_2d, dim_to_ignore_2d = None, None, None, None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load 3d data and load (or create) 2d projections\n",
    "train_set_3d, test_set_3d, data_mean_3d, data_std_3d, dim_to_ignore_3d, dim_to_use_3d, train_root_positions, test_root_positions = data_utils.read_3d_data(\n",
    "    actions, FLAGS.data_dir, FLAGS.camera_frame, rcams, FLAGS.predict_14)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_set_3d.keys()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def set_3D_used_seq(train_set_3d, test_set_3d, train_frame_seq, test_frame_seq):\n",
    "    train_set_3d_new, test_set_3d_new = {}, {}\n",
    "    for key in set(train_frame_seq.keys()) & set(train_set_3d.keys()):\n",
    "        use = np.array(train_frame_seq[key]) - 1\n",
    "        train_set_3d_new[key] = train_set_3d[key][use]\n",
    "    for key in set(test_frame_seq.keys()) & set(test_set_3d.keys()):\n",
    "        use = np.array(test_frame_seq[key]) - 1\n",
    "        test_set_3d_new[key] = test_set_3d[key][use]\n",
    "    return train_set_3d_new, test_set_3d_new\n",
    "\n",
    "train_set_3d, test_set_3d = set_3D_used_seq(train_set_3d, test_set_3d, train_frame_seq, test_frame_seq)\n",
    "train_set_2d, test_set_2d = train_set_2d[train_set_3d.keys()], test_set_2d[test_set_3d.keys()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import data_utils\n",
    "import linear_model_openpose as linear_model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# Avoid using the GPU if requested\n",
    "device_count = {\"GPU\": 0} if FLAGS.use_cpu else {\"GPU\": 1}\n",
    "with tf.Session(config=tf.ConfigProto(\n",
    "        device_count=device_count,\n",
    "        allow_soft_placement=True)) as sess:\n",
    "\n",
    "    # === Create the model ===\n",
    "    print(\"Creating %d bi-layers of %d units.\" % (FLAGS.num_layers, FLAGS.linear_size))\n",
    "    model = create_model(sess, actions, FLAGS.batch_size)\n",
    "    model.train_writer.add_graph(sess.graph)\n",
    "    print(\"Model created\")\n",
    "\n",
    "    # === This is the training loop ===\n",
    "    step_time, loss, val_loss = 0.0, 0.0, 0.0\n",
    "    current_step = 0 if FLAGS.load <= 0 else FLAGS.load + 1\n",
    "    previous_losses = []\n",
    "\n",
    "    step_time, loss = 0, 0\n",
    "    current_epoch = 0\n",
    "    log_every_n_batches = 100\n",
    "\n",
    "    for _ in xrange(FLAGS.epochs):\n",
    "        current_epoch = current_epoch + 1\n",
    "\n",
    "        # === Load training batches for one epoch ===\n",
    "        encoder_inputs, decoder_outputs = model.get_all_batches(train_set_2d, train_set_3d, FLAGS.camera_frame,\n",
    "                                                                training=True)\n",
    "        nbatches = len(encoder_inputs)\n",
    "        print(\"There are {0} train batches\".format(nbatches))\n",
    "        start_time, loss = time.time(), 0.\n",
    "\n",
    "        # === Loop through all the training batches ===\n",
    "        for i in range(nbatches):\n",
    "\n",
    "            if (i + 1) % log_every_n_batches == 0:\n",
    "                # Print progress every log_every_n_batches batches\n",
    "                print(\"Working on epoch {0}, batch {1} / {2}... \".format(current_epoch, i + 1, nbatches), end=\"\")\n",
    "\n",
    "            enc_in, dec_out = encoder_inputs[i], decoder_outputs[i]\n",
    "            step_loss, loss_summary, lr_summary, _ = model.step(sess, enc_in, dec_out, FLAGS.dropout,\n",
    "                                                                isTraining=True)\n",
    "\n",
    "            if (i + 1) % log_every_n_batches == 0:\n",
    "                # Log and print progress every log_every_n_batches batches\n",
    "                model.train_writer.add_summary(loss_summary, current_step)\n",
    "                model.train_writer.add_summary(lr_summary, current_step)\n",
    "                step_time = (time.time() - start_time)\n",
    "                start_time = time.time()\n",
    "                print(\"done in {0:.2f} ms\".format(1000 * step_time / log_every_n_batches))\n",
    "\n",
    "            loss += step_loss\n",
    "            current_step += 1\n",
    "            # === end looping through training batches ===\n",
    "\n",
    "        loss = loss / nbatches\n",
    "        print(\"=============================\\n\"\n",
    "              \"Global step:         %d\\n\"\n",
    "              \"Learning rate:       %.2e\\n\"\n",
    "              \"Train loss avg:      %.4f\\n\"\n",
    "              \"=============================\" % (model.global_step.eval(),\n",
    "                                                 model.learning_rate.eval(), loss))\n",
    "        # === End training for an epoch ===\n",
    "\n",
    "        # === Testing after this epoch ===\n",
    "        isTraining = False\n",
    "\n",
    "        if FLAGS.evaluateActionWise:\n",
    "\n",
    "            print(\"{0:=^12} {1:=^6}\".format(\"Action\", \"mm\"))  # line of 30 equal signs\n",
    "\n",
    "            cum_err = 0\n",
    "            for action in actions:\n",
    "                print(\"{0:<12} \".format(action), end=\"\")\n",
    "                # Get 2d and 3d testing data for this action\n",
    "                action_test_set_2d = get_action_subset(test_set_2d, action)\n",
    "                action_test_set_3d = get_action_subset(test_set_3d, action)\n",
    "                encoder_inputs, decoder_outputs = model.get_all_batches(action_test_set_2d, action_test_set_3d,\n",
    "                                                                        FLAGS.camera_frame, training=False)\n",
    "\n",
    "                act_err, _, step_time, loss = evaluate_batches(sess, model,\n",
    "                                                               data_mean_3d, data_std_3d, dim_to_use_3d,\n",
    "                                                               dim_to_ignore_3d,\n",
    "                                                               data_mean_2d, data_std_2d, dim_to_use_2d,\n",
    "                                                               dim_to_ignore_2d,\n",
    "                                                               current_step, encoder_inputs, decoder_outputs)\n",
    "                cum_err = cum_err + act_err\n",
    "\n",
    "                print(\"{0:>6.2f}\".format(act_err))\n",
    "\n",
    "            summaries = sess.run(model.err_mm_summary, {model.err_mm: float(cum_err / float(len(actions)))})\n",
    "            model.test_writer.add_summary(summaries, current_step)\n",
    "            print(\"{0:<12} {1:>6.2f}\".format(\"Average\", cum_err / float(len(actions))))\n",
    "            print(\"{0:=^19}\".format(''))\n",
    "\n",
    "        else:\n",
    "            n_joints = 13\n",
    "            encoder_inputs, decoder_outputs = model.get_all_batches(test_set_2d, test_set_3d, FLAGS.camera_frame,\n",
    "                                                                    training=False)\n",
    "\n",
    "            total_err, joint_err, step_time, loss = evaluate_batches(sess, model,\n",
    "                                                                     data_mean_3d, data_std_3d, dim_to_use_3d,\n",
    "                                                                     dim_to_ignore_3d,\n",
    "                                                                     data_mean_2d, data_std_2d, dim_to_use_2d,\n",
    "                                                                     dim_to_ignore_2d,\n",
    "                                                                     current_step, encoder_inputs, decoder_outputs,\n",
    "                                                                     current_epoch)\n",
    "\n",
    "            print(\"=============================\\n\"\n",
    "                  \"Step-time (ms):      %.4f\\n\"\n",
    "                  \"Val loss avg:        %.4f\\n\"\n",
    "                  \"Val error avg (mm):  %.2f\\n\"\n",
    "                  \"=============================\" % (1000 * step_time, loss, total_err))\n",
    "\n",
    "            for i in range(n_joints):\n",
    "                # 6 spaces, right-aligned, 5 decimal places\n",
    "                print(\"Error in joint {0:02d} (mm): {1:>5.2f}\".format(i + 1, joint_err[i]))\n",
    "            print(\"=============================\")\n",
    "\n",
    "            # Log the error to tensorboard\n",
    "            summaries = sess.run(model.err_mm_summary, {model.err_mm: total_err})\n",
    "            model.test_writer.add_summary(summaries, current_step)\n",
    "\n",
    "        # Save the model\n",
    "        print(\"Saving the model... \", end=\"\")\n",
    "        start_time = time.time()\n",
    "        model.saver.save(sess, os.path.join(train_dir, 'checkpoint'), global_step=current_step)\n",
    "        print(\"done in {0:.2f} ms\".format(1000 * (time.time() - start_time)))\n",
    "\n",
    "        # Reset global time and loss\n",
    "        step_time, loss = 0, 0\n",
    "\n",
    "        sys.stdout.flush()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_set_2d.keys()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T03:12:15.284487Z",
     "start_time": "2020-11-20T03:12:15.193734Z"
    },
    "code_folding": [
     0
    ],
    "pycharm": {
     "name": "#%%\n"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T03:12:15.607373Z",
     "start_time": "2020-11-20T03:12:15.497254Z"
    },
    "code_folding": [
     0
    ],
    "pycharm": {
     "name": "#%%\n"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def get_action_subset(poses_set, action):\n",
    "    \"\"\"\n",
    "    Given a preloaded dictionary of poses, load the subset of a particular action\n",
    "\n",
    "    Args\n",
    "      poses_set: dictionary with keys k=(subject, action, seqname),\n",
    "        values v=(nxd matrix of poses)\n",
    "      action: string. The action that we want to filter out\n",
    "    Returns\n",
    "      poses_subset: dictionary with same structure as poses_set, but only with the\n",
    "        specified action.\n",
    "    \"\"\"\n",
    "    return {k: v for k, v in poses_set.items() if k[1] == action}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T03:12:16.744417Z",
     "start_time": "2020-11-20T03:12:16.590746Z"
    },
    "code_folding": [
     0
    ],
    "pycharm": {
     "name": "#%%\n"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_batches(sess, model,\n",
    "                     data_mean_3d, data_std_3d, dim_to_use_3d, dim_to_ignore_3d,\n",
    "                     data_mean_2d, data_std_2d, dim_to_use_2d, dim_to_ignore_2d,\n",
    "                     current_step, encoder_inputs, decoder_outputs, current_epoch=0):\n",
    "    \"\"\"\n",
    "    Generic method that evaluates performance of a list of batches.\n",
    "    May be used to evaluate all actions or a single action.\n",
    "\n",
    "    Args\n",
    "      sess\n",
    "      model\n",
    "      data_mean_3d\n",
    "      data_std_3d\n",
    "      dim_to_use_3d\n",
    "      dim_to_ignore_3d\n",
    "      data_mean_2d\n",
    "      data_std_2d\n",
    "      dim_to_use_2d\n",
    "      dim_to_ignore_2d\n",
    "      current_step\n",
    "      encoder_inputs\n",
    "      decoder_outputs\n",
    "      current_epoch\n",
    "    Returns\n",
    "\n",
    "      total_err\n",
    "      joint_err\n",
    "      step_time\n",
    "      loss\n",
    "    \"\"\"\n",
    "\n",
    "    # n_joints = 17 if not (FLAGS.predict_14) else 14\n",
    "    n_joints = 13\n",
    "    nbatches = len(encoder_inputs)\n",
    "\n",
    "    # Loop through test examples\n",
    "    all_dists, start_time, loss = [], time.time(), 0.\n",
    "    log_every_n_batches = 100\n",
    "    for i in range(nbatches):\n",
    "\n",
    "        if current_epoch > 0 and (i + 1) % log_every_n_batches == 0:\n",
    "            print(\"Working on test epoch {0}, batch {1} / {2}\".format(current_epoch, i + 1, nbatches))\n",
    "\n",
    "        enc_in, dec_out = encoder_inputs[i], decoder_outputs[i]\n",
    "        dp = 1.0  # dropout keep probability is always 1 at test time\n",
    "        step_loss, loss_summary, poses3d = model.step(sess, enc_in, dec_out, dp, isTraining=False)\n",
    "        loss += step_loss\n",
    "\n",
    "        # denormalize\n",
    "        # enc_in = data_utils.unNormalizeData(enc_in, data_mean_2d, data_std_2d, dim_to_ignore_2d)\n",
    "        dec_out = data_utils.unNormalizeData(dec_out, data_mean_3d, data_std_3d, dim_to_ignore_3d)\n",
    "        poses3d = data_utils.unNormalizeData(poses3d, data_mean_3d, data_std_3d, dim_to_ignore_3d)\n",
    "\n",
    "        # Keep only the relevant dimensions\n",
    "        dtu3d = np.hstack((np.arange(3), dim_to_use_3d)) if not (FLAGS.predict_14) else dim_to_use_3d\n",
    "\n",
    "        dec_out = dec_out[:, dtu3d]\n",
    "        poses3d = poses3d[:, dtu3d]\n",
    "\n",
    "        assert dec_out.shape[0] == FLAGS.batch_size\n",
    "        assert poses3d.shape[0] == FLAGS.batch_size\n",
    "\n",
    "        if FLAGS.procrustes:\n",
    "            # Apply per-frame procrustes alignment if asked to do so\n",
    "            for j in range(FLAGS.batch_size):\n",
    "                gt = np.reshape(dec_out[j, :], [-1, 3])\n",
    "                out = np.reshape(poses3d[j, :], [-1, 3])\n",
    "                _, Z, T, b, c = procrustes.compute_similarity_transform(gt, out, compute_optimal_scale=True)\n",
    "                out = (b * out.dot(T)) + c\n",
    "\n",
    "                # poses3d[j, :] = np.reshape(out, [-1, 17 * 3]) if not (FLAGS.predict_14) else np.reshape(out,\n",
    "                #                                                                                         [-1, 14 * 3])\n",
    "                poses3d[j, :] = np.reshape(out, [-1, 13 * 3])\n",
    "\n",
    "        # Compute Euclidean distance error per joint\n",
    "        sqerr = (poses3d - dec_out) ** 2  # Squared error between prediction and expected output\n",
    "        dists = np.zeros((sqerr.shape[0], n_joints))  # Array with L2 error per joint in mm\n",
    "        dist_idx = 0\n",
    "        for k in np.arange(0, n_joints * 3, 3):\n",
    "            # Sum across X,Y, and Z dimenstions to obtain L2 distance\n",
    "            dists[:, dist_idx] = np.sqrt(np.sum(sqerr[:, k:k + 3], axis=1))\n",
    "            dist_idx = dist_idx + 1\n",
    "\n",
    "        all_dists.append(dists)\n",
    "        assert sqerr.shape[0] == FLAGS.batch_size\n",
    "\n",
    "    step_time = (time.time() - start_time) / nbatches\n",
    "    loss = loss / nbatches\n",
    "\n",
    "    all_dists = np.vstack(all_dists)\n",
    "\n",
    "    # Error per joint and total for all passed batches\n",
    "    joint_err = np.mean(all_dists, axis=0)\n",
    "    total_err = np.mean(all_dists)\n",
    "\n",
    "    return total_err, joint_err, step_time, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T03:24:11.557853Z",
     "start_time": "2020-11-20T03:24:11.376168Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "actions = data_utils.define_actions(FLAGS.action)\n",
    "\n",
    "# Load camera parameters\n",
    "SUBJECT_IDS = [1, 5, 6, 7, 8, 9, 11]\n",
    "rcams = cameras.load_cameras(FLAGS.cameras_path, SUBJECT_IDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T03:07:47.565072Z",
     "start_time": "2020-11-20T03:07:34.818547Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# train_set_2d, test_set_2d, data_mean_2d, data_std_2d, dim_to_ignore_2d, dim_to_use_2d = data_utils.create_openpose_2D_data(\n",
    "#     actions, FLAGS.csv_dir, rcams)\n",
    "train_set_2d, test_set_2d, train_frame_seq, test_frame_seq = data_utils.create_openpose_2D_data(actions,\n",
    "                                                                                                FLAGS.csv_dir,\n",
    "                                                                                                rcams)\n",
    "data_mean_2d, data_std_2d, dim_to_use_2d, dim_to_ignore_2d = None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T03:24:18.195921Z",
     "start_time": "2020-11-20T03:24:16.166246Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Load 3d data and load (or create) 2d projections\n",
    "train_set_3d, test_set_3d, data_mean_3d, data_std_3d, dim_to_ignore_3d, dim_to_use_3d, train_root_positions, test_root_positions = data_utils.read_3d_data(\n",
    "    actions, FLAGS.data_dir, FLAGS.camera_frame, rcams, FLAGS.predict_14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T03:27:21.951873Z",
     "start_time": "2020-11-20T03:27:21.740336Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "train_set_3d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T03:09:27.107838Z",
     "start_time": "2020-11-20T03:09:25.481116Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def set_3D_used_seq(train_set_3d, test_set_3d, train_frame_seq, test_frame_seq):\n",
    "    train_set_3d_new, test_set_3d_new = {}, {}\n",
    "    for key in set(train_frame_seq.keys()) & set(train_set_3d.keys()):\n",
    "        use = np.array(train_frame_seq[key]) - 1\n",
    "        train_set_3d_new[key] = train_set_3d[key][use]\n",
    "    for key in set(test_frame_seq.keys()) & set(test_set_3d.keys()):\n",
    "        use = np.array(test_frame_seq[key]) - 1\n",
    "        test_set_3d_new[key] = test_set_3d[key][use]\n",
    "    return train_set_3d_new, test_set_3d_new\n",
    "\n",
    "train_set_3d, test_set_3d = set_3D_used_seq(train_set_3d, test_set_3d, train_frame_seq, test_frame_seq)\n",
    "train_set_2d, test_set_2d = train_set_2d[train_set_3d.keys()], test_set_2d[test_set_3d.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T03:08:33.025519Z",
     "start_time": "2020-11-20T03:08:32.962195Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import data_utils\n",
    "import linear_model_openpose as linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T02:26:28.432239Z",
     "start_time": "2020-11-20T02:26:26.899709Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# Avoid using the GPU if requested\n",
    "device_count = {\"GPU\": 0} if FLAGS.use_cpu else {\"GPU\": 1}\n",
    "with tf.Session(config=tf.ConfigProto(\n",
    "        device_count=device_count,\n",
    "        allow_soft_placement=True)) as sess:\n",
    "\n",
    "    # === Create the model ===\n",
    "    print(\"Creating %d bi-layers of %d units.\" % (FLAGS.num_layers, FLAGS.linear_size))\n",
    "    model = create_model(sess, actions, FLAGS.batch_size)\n",
    "    model.train_writer.add_graph(sess.graph)\n",
    "    print(\"Model created\")\n",
    "\n",
    "    # === This is the training loop ===\n",
    "    step_time, loss, val_loss = 0.0, 0.0, 0.0\n",
    "    current_step = 0 if FLAGS.load <= 0 else FLAGS.load + 1\n",
    "    previous_losses = []\n",
    "\n",
    "    step_time, loss = 0, 0\n",
    "    current_epoch = 0\n",
    "    log_every_n_batches = 100\n",
    "\n",
    "    for _ in xrange(FLAGS.epochs):\n",
    "        current_epoch = current_epoch + 1\n",
    "\n",
    "        # === Load training batches for one epoch ===\n",
    "        encoder_inputs, decoder_outputs = model.get_all_batches(train_set_2d, train_set_3d, FLAGS.camera_frame,\n",
    "                                                                training=True)\n",
    "        nbatches = len(encoder_inputs)\n",
    "        print(\"There are {0} train batches\".format(nbatches))\n",
    "        start_time, loss = time.time(), 0.\n",
    "\n",
    "        # === Loop through all the training batches ===\n",
    "        for i in range(nbatches):\n",
    "\n",
    "            if (i + 1) % log_every_n_batches == 0:\n",
    "                # Print progress every log_every_n_batches batches\n",
    "                print(\"Working on epoch {0}, batch {1} / {2}... \".format(current_epoch, i + 1, nbatches), end=\"\")\n",
    "\n",
    "            enc_in, dec_out = encoder_inputs[i], decoder_outputs[i]\n",
    "            step_loss, loss_summary, lr_summary, _ = model.step(sess, enc_in, dec_out, FLAGS.dropout,\n",
    "                                                                isTraining=True)\n",
    "\n",
    "            if (i + 1) % log_every_n_batches == 0:\n",
    "                # Log and print progress every log_every_n_batches batches\n",
    "                model.train_writer.add_summary(loss_summary, current_step)\n",
    "                model.train_writer.add_summary(lr_summary, current_step)\n",
    "                step_time = (time.time() - start_time)\n",
    "                start_time = time.time()\n",
    "                print(\"done in {0:.2f} ms\".format(1000 * step_time / log_every_n_batches))\n",
    "\n",
    "            loss += step_loss\n",
    "            current_step += 1\n",
    "            # === end looping through training batches ===\n",
    "\n",
    "        loss = loss / nbatches\n",
    "        print(\"=============================\\n\"\n",
    "              \"Global step:         %d\\n\"\n",
    "              \"Learning rate:       %.2e\\n\"\n",
    "              \"Train loss avg:      %.4f\\n\"\n",
    "              \"=============================\" % (model.global_step.eval(),\n",
    "                                                 model.learning_rate.eval(), loss))\n",
    "        # === End training for an epoch ===\n",
    "\n",
    "        # === Testing after this epoch ===\n",
    "        isTraining = False\n",
    "\n",
    "        if FLAGS.evaluateActionWise:\n",
    "\n",
    "            print(\"{0:=^12} {1:=^6}\".format(\"Action\", \"mm\"))  # line of 30 equal signs\n",
    "\n",
    "            cum_err = 0\n",
    "            for action in actions:\n",
    "                print(\"{0:<12} \".format(action), end=\"\")\n",
    "                # Get 2d and 3d testing data for this action\n",
    "                action_test_set_2d = get_action_subset(test_set_2d, action)\n",
    "                action_test_set_3d = get_action_subset(test_set_3d, action)\n",
    "                encoder_inputs, decoder_outputs = model.get_all_batches(action_test_set_2d, action_test_set_3d,\n",
    "                                                                        FLAGS.camera_frame, training=False)\n",
    "\n",
    "                act_err, _, step_time, loss = evaluate_batches(sess, model,\n",
    "                                                               data_mean_3d, data_std_3d, dim_to_use_3d,\n",
    "                                                               dim_to_ignore_3d,\n",
    "                                                               data_mean_2d, data_std_2d, dim_to_use_2d,\n",
    "                                                               dim_to_ignore_2d,\n",
    "                                                               current_step, encoder_inputs, decoder_outputs)\n",
    "                cum_err = cum_err + act_err\n",
    "\n",
    "                print(\"{0:>6.2f}\".format(act_err))\n",
    "\n",
    "            summaries = sess.run(model.err_mm_summary, {model.err_mm: float(cum_err / float(len(actions)))})\n",
    "            model.test_writer.add_summary(summaries, current_step)\n",
    "            print(\"{0:<12} {1:>6.2f}\".format(\"Average\", cum_err / float(len(actions))))\n",
    "            print(\"{0:=^19}\".format(''))\n",
    "\n",
    "        else:\n",
    "            n_joints = 13\n",
    "            encoder_inputs, decoder_outputs = model.get_all_batches(test_set_2d, test_set_3d, FLAGS.camera_frame,\n",
    "                                                                    training=False)\n",
    "\n",
    "            total_err, joint_err, step_time, loss = evaluate_batches(sess, model,\n",
    "                                                                     data_mean_3d, data_std_3d, dim_to_use_3d,\n",
    "                                                                     dim_to_ignore_3d,\n",
    "                                                                     data_mean_2d, data_std_2d, dim_to_use_2d,\n",
    "                                                                     dim_to_ignore_2d,\n",
    "                                                                     current_step, encoder_inputs, decoder_outputs,\n",
    "                                                                     current_epoch)\n",
    "\n",
    "            print(\"=============================\\n\"\n",
    "                  \"Step-time (ms):      %.4f\\n\"\n",
    "                  \"Val loss avg:        %.4f\\n\"\n",
    "                  \"Val error avg (mm):  %.2f\\n\"\n",
    "                  \"=============================\" % (1000 * step_time, loss, total_err))\n",
    "\n",
    "            for i in range(n_joints):\n",
    "                # 6 spaces, right-aligned, 5 decimal places\n",
    "                print(\"Error in joint {0:02d} (mm): {1:>5.2f}\".format(i + 1, joint_err[i]))\n",
    "            print(\"=============================\")\n",
    "\n",
    "            # Log the error to tensorboard\n",
    "            summaries = sess.run(model.err_mm_summary, {model.err_mm: total_err})\n",
    "            model.test_writer.add_summary(summaries, current_step)\n",
    "\n",
    "        # Save the model\n",
    "        print(\"Saving the model... \", end=\"\")\n",
    "        start_time = time.time()\n",
    "        model.saver.save(sess, os.path.join(train_dir, 'checkpoint'), global_step=current_step)\n",
    "        print(\"done in {0:.2f} ms\".format(1000 * (time.time() - start_time)))\n",
    "\n",
    "        # Reset global time and loss\n",
    "        step_time, loss = 0, 0\n",
    "\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T02:28:57.450584Z",
     "start_time": "2020-11-20T02:28:57.363120Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "train_set_2d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}